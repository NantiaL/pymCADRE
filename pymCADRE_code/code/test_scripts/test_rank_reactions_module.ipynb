{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cobra import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb981df",
   "metadata": {},
   "source": [
    "### Test parse_gprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank.parse_gprs import *\n",
    "print('## Testing parse_gprs..')\n",
    "\n",
    "model = io.read_sbml_model('../pre_processing/dataset/recon1_with_BOF_and_VBOF.xml')\n",
    "\n",
    "try:\n",
    "\n",
    "    parseGPRS = parse_gprs(model)\n",
    "    \n",
    "    GPR_rxns = parseGPRS[0]\n",
    "    GPR_file = parseGPRS[1]\n",
    "    \n",
    "    if (len(GPR_rxns) != 5962) and (len(GPR_file) != 5962):\n",
    "        \n",
    "        print('Function returns the [unexpected] result')\n",
    "\n",
    "    \n",
    "    print('PASS...', 'Function parse_gprs ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function parse_gprs was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ebb94",
   "metadata": {},
   "source": [
    "### Test map_high_conf_to_rxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6607be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank.map_high_conf_to_rxns import *\n",
    "print('## Testing map_high_conf_to_rxns..')\n",
    "\n",
    "try:\n",
    "\n",
    "    is_C_H = map_high_conf_to_rxns(model, GPR_file, GPR_rxns, [])\n",
    "    \n",
    "    \n",
    "    if len(is_C_H) != 3743:\n",
    "        \n",
    "        print('Function returns the [unexpected] result:' , len(is_C_H))\n",
    "\n",
    "    \n",
    "    print('PASS...', 'Function map_high_conf_to_rxns ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function map_high_conf_to_rxns was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981f623e",
   "metadata": {},
   "source": [
    "### Test map_gene_scores_to_rxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank.map_gene_scores_to_rxns import *\n",
    "\n",
    "# genes\n",
    "G = pd.read_csv('../pre_processing/dataset/1_GPL570_GSE3397/1_GPL570_GSE3397_entrez_ids.csv')\n",
    "G = list(G['ENTREZ_GENE_ID'])\n",
    "# ubiquity scores\n",
    "U = pd.read_csv('../pre_processing/dataset/1_GPL570_GSE3397/1_GPL570_GSE3397_ubiquity.csv', header=None)\n",
    "U = U.rename(columns={0: \"Scores\"})\n",
    "U = list(U['Scores'])\n",
    "\n",
    "print('## Testing map_gene_scores_to_rxns..')\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    U_GPR = map_gene_scores_to_rxns(model, G, U, GPR_file)\n",
    "    \n",
    "    \n",
    "    if len(U_GPR) != 5962:\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(U_GPR))\n",
    "\n",
    "    \n",
    "    print('PASS...', 'Function map_gene_scores_to_rxns ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function map_gene_scores_to_rxns was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28695cab",
   "metadata": {},
   "source": [
    "### Test calc_expr_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank.calc_expr_evidence import *\n",
    "\n",
    "print('## Testing calc_expr_evidence..')\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    E_X = calc_expr_evidence(model, GPR_rxns, U_GPR,is_C_H)\n",
    "    \n",
    "    \n",
    "    if len(E_X) != 3743:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(E_X))\n",
    "\n",
    "    \n",
    "    print('PASS...', 'Function calc_expr_evidence ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function calc_expr_evidence was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61006330",
   "metadata": {},
   "source": [
    "### Test initialize_generic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6212db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "for i in range(len(E_X)):\n",
    "    if E_X[i] > 0.9:\n",
    "        C.append(model.reactions[i].id)\n",
    "confidence_scores = pd.read_csv('../pre_processing/dataset/Recon1_confidence_scores_with_BOF_and_VBOF.csv')\n",
    "confidence_scores = np.float64(list(confidence_scores['Confidence Score']))\n",
    "\n",
    "from rank.initialize_generic_model import *\n",
    "\n",
    "print('## Testing initialize_generic_model..')\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    GM, C, E_X, E_L = initialize_generic_model(model,C,E_X,confidence_scores,1)\n",
    "    \n",
    "    \n",
    "    if len(GM.reactions) != 2471:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(GM.reactions))\n",
    "        \n",
    "        \n",
    "    elif len(C) != 860:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(C))\n",
    "    \n",
    "    \n",
    "    elif len(E_X) != 2471:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(E_X))\n",
    "        \n",
    "    elif len(E_L) != 2471:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(E_L))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print('Function returns the expected result')\n",
    "    \n",
    "    print('PASS...', 'Function initialize_generic_model ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function initialize_generic_model was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6a532",
   "metadata": {},
   "source": [
    "### Test rank_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank.rank_reactions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read model\n",
    "model = io.read_sbml_model('../pre_processing/dataset/recon1_with_BOF_and_VBOF.xml')\n",
    "# genes\n",
    "G = pd.read_csv('../pre_processing/dataset/1_GPL570_GSE3397/1_GPL570_GSE3397_entrez_ids.csv')\n",
    "G = list(G['ENTREZ_GENE_ID'])\n",
    "# ubiquity scores\n",
    "U = pd.read_csv('../pre_processing/dataset/1_GPL570_GSE3397/1_GPL570_GSE3397_ubiquity.csv', header=None)\n",
    "U = U.rename(columns={0: \"Scores\"})\n",
    "U = list(U['Scores'])\n",
    "#confidence_scores\n",
    "#confidence_scores = get_test_inputs('../testInputs.mat','../humanModel.mat')[3]\n",
    "#confidence_scores[-1] = 4\n",
    "confidence_scores = pd.read_csv('../pre_processing/dataset/Recon1_confidence_scores_with_BOF_and_VBOF.csv')\n",
    "confidence_scores = np.float64(list(confidence_scores['Confidence Score']))\n",
    "\n",
    "print('## Testing rank_reactions..')\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    GM, C, NC, P, Z, model_C = rank_reactions(model, G, U, confidence_scores, [], 1)\n",
    "    \n",
    "    if len(GM.reactions) != 2471:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(GM.reactions))\n",
    "        \n",
    "        \n",
    "    elif len(C) != 860:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(C))\n",
    "    \n",
    "    \n",
    "    elif len(NC) != 1611:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(NC))\n",
    "        \n",
    "    elif len(P) != 1611:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(P))\n",
    "\n",
    "    elif len(Z) != 473:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(Z))\n",
    "    \n",
    "      \n",
    "    elif len(model_C) != 1322:\n",
    "\n",
    "\n",
    "        print('Function returns the [unexpected] result', len(model_C))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Function returns the expected result')\n",
    "    \n",
    "    print('PASS...', 'Function rank_reactions ran without error')\n",
    "\n",
    "except (RuntimeError, TypeError, NameError) as inst:\n",
    "\n",
    "    print('FAIL...', 'Function rank_reactions was terminated with the error:')\n",
    "    print(type(inst))\n",
    "    print(inst.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ea634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
